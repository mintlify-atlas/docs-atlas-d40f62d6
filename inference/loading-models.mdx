---
title: 'Loading Models'
description: 'Load checkpoints and pretrained GPT-2 models for inference'
---

## Overview

nanoGPT supports two primary ways to load models: from your own trained checkpoints or from OpenAI's pretrained GPT-2 models via the Hugging Face transformers library.

## Loading from Checkpoints

### Resume Mode

Load a model from a checkpoint directory using `init_from='resume'`:

```python sample.py:35-46
if init_from == 'resume':
    # init from a model saved in a specific directory
    ckpt_path = os.path.join(out_dir, 'ckpt.pt')
    checkpoint = torch.load(ckpt_path, map_location=device)
    gptconf = GPTConfig(**checkpoint['model_args'])
    model = GPT(gptconf)
    state_dict = checkpoint['model']
    unwanted_prefix = '_orig_mod.'
    for k,v in list(state_dict.items()):
        if k.startswith(unwanted_prefix):
            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)
    model.load_state_dict(state_dict)
```

<Steps>
  <Step title="Load Checkpoint File">
    The checkpoint is loaded from `out_dir/ckpt.pt` containing model weights and configuration.
  </Step>

  <Step title="Reconstruct Configuration">
    Model architecture parameters are extracted from `checkpoint['model_args']` and used to create a matching `GPTConfig`.
  </Step>

  <Step title="Handle torch.compile Artifacts">
    Remove the `_orig_mod.` prefix added by PyTorch 2.0's `torch.compile()` to ensure compatibility.
  </Step>

  <Step title="Load State Dict">
    Load the model weights into the newly created model instance.
  </Step>
</Steps>

### Command Line Usage

```bash
python sample.py --out_dir=out-shakespeare-char
```

By default, `init_from` is set to `'resume'`, so this automatically loads from your checkpoint.

## Loading Pretrained GPT-2 Models

### The from_pretrained Method

Load any of OpenAI's GPT-2 variants directly:

```python sample.py:47-49
elif init_from.startswith('gpt2'):
    # init from a given GPT-2 model
    model = GPT.from_pretrained(init_from, dict(dropout=0.0))
```

The `from_pretrained` method in `model.py` handles the weight conversion:

```python model.py:206-261
@classmethod
def from_pretrained(cls, model_type, override_args=None):
    assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}
    override_args = override_args or {} # default to empty dict
    # only dropout can be overridden see more notes below
    assert all(k == 'dropout' for k in override_args)
    from transformers import GPT2LMHeadModel
    print("loading weights from pretrained gpt: %s" % model_type)

    # n_layer, n_head and n_embd are determined from model_type
    config_args = {
        'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params
        'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params
        'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params
        'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params
    }[model_type]
    print("forcing vocab_size=50257, block_size=1024, bias=True")
    config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints
    config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints
    config_args['bias'] = True # always True for GPT model checkpoints
    # we can override the dropout rate, if desired
    if 'dropout' in override_args:
        print(f"overriding dropout rate to {override_args['dropout']}")
        config_args['dropout'] = override_args['dropout']
    # create a from-scratch initialized minGPT model
    config = GPTConfig(**config_args)
    model = GPT(config)
    sd = model.state_dict()
    sd_keys = sd.keys()
    sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param

    # init a huggingface/transformers model
    model_hf = GPT2LMHeadModel.from_pretrained(model_type)
    sd_hf = model_hf.state_dict()

    # copy while ensuring all of the parameters are aligned and match in names and shapes
    sd_keys_hf = sd_hf.keys()
    sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer
    sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)
    transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']
    # basically the openai checkpoints use a "Conv1D" module, but we only want to use a vanilla Linear
    # this means that we have to transpose these weights when we import them
    assert len(sd_keys_hf) == len(sd_keys), f"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}"
    for k in sd_keys_hf:
        if any(k.endswith(w) for w in transposed):
            # special treatment for the Conv1D weights we need to transpose
            assert sd_hf[k].shape[::-1] == sd[k].shape
            with torch.no_grad():
                sd[k].copy_(sd_hf[k].t())
        else:
            # vanilla copy over the other parameters
            assert sd_hf[k].shape == sd[k].shape
            with torch.no_grad():
                sd[k].copy_(sd_hf[k])

    return model
```

### Available Model Sizes

| Model | Parameters | Layers | Heads | Embedding Size |
|-------|-----------|---------|-------|----------------|
| `gpt2` | 124M | 12 | 12 | 768 |
| `gpt2-medium` | 350M | 24 | 16 | 1024 |
| `gpt2-large` | 774M | 36 | 20 | 1280 |
| `gpt2-xl` | 1558M | 48 | 25 | 1600 |

### Command Line Usage

```bash
python sample.py --init_from=gpt2-xl
```

<Note>
Pretrained models use OpenAI's BPE tokenizer (via tiktoken) with vocab size 50257 and context length 1024.
</Note>

## Checkpoint Structure

Checkpoints saved during training contain:

```python
checkpoint = {
    'model': model.state_dict(),
    'optimizer': optimizer.state_dict(),
    'model_args': model_args,
    'iter_num': iter_num,
    'best_val_loss': best_val_loss,
    'config': config,
}
```

Key components:
- **model**: The model weights (state_dict)
- **model_args**: Architecture configuration (n_layer, n_head, n_embd, etc.)
- **optimizer**: Optimizer state for resuming training
- **iter_num**: Training iteration number
- **best_val_loss**: Best validation loss achieved
- **config**: Training configuration including dataset name

## Model Surgery: Cropping Block Size

You can load a pretrained GPT-2 model and reduce its context size:

```python model.py:195-204
def crop_block_size(self, block_size):
    # model surgery to decrease the block size if necessary
    # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)
    # but want to use a smaller block size for some smaller, simpler model
    assert block_size <= self.config.block_size
    self.config.block_size = block_size
    self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])
    for block in self.transformer.h:
        if hasattr(block.attn, 'bias'):
            block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]
```

This is useful when:
- You need shorter context for memory constraints
- You're finetuning on data with shorter sequences
- You want faster inference

<Tip>
When loading pretrained models for inference, set `dropout=0.0` in the override args to disable dropout.
</Tip>

## Tokenizer Handling

The sampling script automatically handles tokenization:

<Steps>
  <Step title="Check for Custom Tokenizer">
    If resuming from a checkpoint, look for `meta.pkl` in the dataset directory containing custom character-level encoders.
  </Step>

  <Step title="Fall Back to GPT-2 BPE">
    If no custom tokenizer found, use tiktoken's GPT-2 encoding:
    
    ```python sample.py:69-74
    else:
        # ok let's assume gpt-2 encodings by default
        print("No meta.pkl found, assuming GPT-2 encodings...")
        enc = tiktoken.get_encoding("gpt2")
        encode = lambda s: enc.encode(s, allowed_special={"<|endoftext|>"})
        decode = lambda l: enc.decode(l)
    ```
  </Step>
</Steps>