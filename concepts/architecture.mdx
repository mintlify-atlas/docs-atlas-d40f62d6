---
title: 'Model Architecture'
description: 'Understanding the GPT model architecture in nanoGPT'
---

## Overview

nanoGPT implements a decoder-only transformer architecture based on the original GPT-2 design. The entire model is defined in a single file (`model.py`) for simplicity and clarity.

## GPTConfig Dataclass

The model configuration is defined using a Python dataclass with the following parameters:

```python model.py:108-116
@dataclass
class GPTConfig:
    block_size: int = 1024
    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency
    n_layer: int = 12
    n_head: int = 12
    n_embd: int = 768
    dropout: float = 0.0
    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster
```

<Accordion title="Configuration Parameters Explained">
- **block_size**: Maximum sequence length (context window)
- **vocab_size**: Size of the vocabulary (number of unique tokens)
- **n_layer**: Number of transformer blocks
- **n_head**: Number of attention heads per block
- **n_embd**: Embedding dimensionality
- **dropout**: Dropout rate for regularization
- **bias**: Whether to use bias in Linear and LayerNorm layers
</Accordion>

## Model Components

The GPT model is built from several key components:

### 1. LayerNorm

Custom LayerNorm implementation with optional bias:

```python model.py:18-27
class LayerNorm(nn.Module):
    """ LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False """

    def __init__(self, ndim, bias):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(ndim))
        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None

    def forward(self, input):
        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)
```

### 2. CausalSelfAttention

The attention mechanism with Flash Attention support:

```python model.py:29-51
class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        assert config.n_embd % config.n_head == 0
        # key, query, value projections for all heads, but in a batch
        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)
        # output projection
        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)
        # regularization
        self.attn_dropout = nn.Dropout(config.dropout)
        self.resid_dropout = nn.Dropout(config.dropout)
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.dropout = config.dropout
        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0
        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')
        if not self.flash:
            print("WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0")
            # causal mask to ensure that attention is only applied to the left in the input sequence
            self.register_buffer("bias", torch.tril(torch.ones(config.block_size, config.block_size))
                                        .view(1, 1, config.block_size, config.block_size))
```

<Info>
The attention mechanism automatically detects PyTorch >= 2.0 and uses Flash Attention when available for significant performance improvements.
</Info>

The forward pass implements both Flash Attention and manual attention:

```python model.py:62-72
if self.flash:
    # efficient attention using Flash Attention CUDA kernels
    y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)
else:
    # manual implementation of attention
    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
    att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
    att = F.softmax(att, dim=-1)
    att = self.attn_dropout(att)
    y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)
y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side
```

### 3. MLP (Feed-Forward Network)

The feed-forward network with GELU activation:

```python model.py:78-92
class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)
        self.gelu    = nn.GELU()
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)
        self.dropout = nn.Dropout(config.dropout)

    def forward(self, x):
        x = self.c_fc(x)
        x = self.gelu(x)
        x = self.c_proj(x)
        x = self.dropout(x)
        return x
```

<Note>
The MLP expands the embedding dimension by 4x internally, following the standard transformer architecture.
</Note>

### 4. Transformer Block

A single transformer block combines attention and MLP with residual connections:

```python model.py:94-106
class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)
        self.attn = CausalSelfAttention(config)
        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(self.ln_1(x))
        x = x + self.mlp(self.ln_2(x))
        return x
```

The block uses pre-normalization (LayerNorm before attention/MLP) with residual connections.

## Full GPT Model

The complete model architecture:

```python model.py:126-133
self.transformer = nn.ModuleDict(dict(
    wte = nn.Embedding(config.vocab_size, config.n_embd),
    wpe = nn.Embedding(config.block_size, config.n_embd),
    drop = nn.Dropout(config.dropout),
    h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
    ln_f = LayerNorm(config.n_embd, bias=config.bias),
))
self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
```

<Accordion title="Model Components">
- **wte**: Token embeddings
- **wpe**: Position embeddings
- **drop**: Dropout layer
- **h**: Stack of transformer blocks
- **ln_f**: Final layer normalization
- **lm_head**: Language modeling head (projects to vocabulary)
</Accordion>

### Weight Tying

The model uses weight tying between token embeddings and the output layer:

```python model.py:138
self.transformer.wte.weight = self.lm_head.weight
```

This reduces parameters and often improves performance.

## Pre-trained Model Sizes

nanoGPT supports loading pre-trained GPT-2 models in four sizes:

<Tabs>
  <Tab title="gpt2 (124M)">
    ```python
    config_args = dict(n_layer=12, n_head=12, n_embd=768)
    ```
    - 12 layers, 12 attention heads
    - 768 embedding dimensions
    - ~124M parameters
  </Tab>
  
  <Tab title="gpt2-medium (350M)">
    ```python
    config_args = dict(n_layer=24, n_head=16, n_embd=1024)
    ```
    - 24 layers, 16 attention heads
    - 1024 embedding dimensions
    - ~350M parameters
  </Tab>
  
  <Tab title="gpt2-large (774M)">
    ```python
    config_args = dict(n_layer=36, n_head=20, n_embd=1280)
    ```
    - 36 layers, 20 attention heads
    - 1280 embedding dimensions
    - ~774M parameters
  </Tab>
  
  <Tab title="gpt2-xl (1.5B)">
    ```python
    config_args = dict(n_layer=48, n_head=25, n_embd=1600)
    ```
    - 48 layers, 25 attention heads
    - 1600 embedding dimensions
    - ~1558M parameters
  </Tab>
</Tabs>

## Loading Pre-trained Models

Load a pre-trained GPT-2 model:

```python
model = GPT.from_pretrained('gpt2')  # or gpt2-medium, gpt2-large, gpt2-xl
```

The `from_pretrained` method (model.py:206-261) loads weights from HuggingFace's transformers library and converts them to nanoGPT's format.

## Model Forward Pass

The forward method processes input tokens:

```python model.py:170-193
def forward(self, idx, targets=None):
    device = idx.device
    b, t = idx.size()
    assert t <= self.config.block_size, f"Cannot forward sequence of length {t}, block size is only {self.config.block_size}"
    pos = torch.arange(0, t, dtype=torch.long, device=device)

    # forward the GPT model itself
    tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
    pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)
    x = self.transformer.drop(tok_emb + pos_emb)
    for block in self.transformer.h:
        x = block(x)
    x = self.transformer.ln_f(x)

    if targets is not None:
        # if we are given some desired targets also calculate the loss
        logits = self.lm_head(x)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
    else:
        # inference-time mini-optimization: only forward the lm_head on the very last position
        logits = self.lm_head(x[:, [-1], :])
        loss = None

    return logits, loss
```

<Tip>
During inference, the model only computes the language modeling head on the last position for efficiency.
</Tip>

## Model Surgery

You can reduce the block size dynamically:

```python model.py:195-204
def crop_block_size(self, block_size):
    # model surgery to decrease the block size if necessary
    # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)
    # but want to use a smaller block size for some smaller, simpler model
    assert block_size <= self.config.block_size
    self.config.block_size = block_size
    self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])
    for block in self.transformer.h:
        if hasattr(block.attn, 'bias'):
            block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]
```

## Parameter Count

The model reports its parameter count on initialization:

```python model.py:150-160
def get_num_params(self, non_embedding=True):
    """
    Return the number of parameters in the model.
    For non-embedding count (default), the position embeddings get subtracted.
    The token embeddings would too, except due to the parameter sharing these
    params are actually used as weights in the final layer, so we include them.
    """
    n_params = sum(p.numel() for p in self.parameters())
    if non_embedding:
        n_params -= self.transformer.wpe.weight.numel()
    return n_params
```
