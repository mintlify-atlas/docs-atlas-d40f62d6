---
title: Introduction
description: The simplest, fastest repository for training/finetuning medium-sized GPTs
---

# Welcome to nanoGPT

nanoGPT is the simplest, fastest repository for training and finetuning medium-sized GPT models. It's a rewrite of [minGPT](https://github.com/karpathy/minGPT) that prioritizes **teeth over education** - focusing on practical performance rather than educational clarity.

<Warning>
**Update Nov 2025:** nanoGPT has a new and improved cousin called [nanochat](https://github.com/karpathy/nanochat). It is very likely you meant to use/find nanochat instead. nanoGPT (this repo) is now very old and deprecated but has been left up for posterity.
</Warning>

## What is nanoGPT?

nanoGPT is a minimalist yet powerful implementation for training GPT models from scratch or finetuning pretrained checkpoints. The entire codebase consists of just two ~300-line files:

- `train.py` - A clean boilerplate training loop
- `model.py` - A complete GPT model definition with optional GPT-2 weight loading

Despite its simplicity, nanoGPT successfully **reproduces GPT-2 (124M)** on OpenWebText, running on a single 8Ã—A100 40GB node in about 4 days of training.

## Key Features

<CardGroup cols={2}>
  <Card title="Simple & Readable" icon="code">
    Just ~600 lines of clean, hackable Python code split across two main files
  </Card>
  
  <Card title="GPT-2 Reproduction" icon="clone">
    Successfully reproduces GPT-2 (124M) results on OpenWebText dataset
  </Card>
  
  <Card title="Distributed Training" icon="server">
    Built-in support for PyTorch DDP across multiple GPUs and nodes
  </Card>
  
  <Card title="Fast Performance" icon="bolt">
    Leverages PyTorch 2.0 compile and Flash Attention for optimal speed
  </Card>
  
  <Card title="Flexible Finetuning" icon="sliders">
    Easy to finetune pretrained checkpoints (GPT-2, GPT-2 Medium, Large, XL)
  </Card>
  
  <Card title="Character & BPE" icon="font">
    Supports both character-level and BPE tokenization
  </Card>
</CardGroup>

## Why nanoGPT?

<Note>
Because the code is so simple, it is very easy to hack to your needs, train new models from scratch, or finetune pretrained checkpoints.
</Note>

The minimalist design means you can:

- **Understand** the entire codebase in an afternoon
- **Modify** any aspect without navigating complex abstractions
- **Experiment** rapidly with new ideas and architectures
- **Debug** issues quickly with transparent, readable code

## Performance Benchmarks

OpenAI GPT-2 checkpoints evaluated on OpenWebText:

| Model | Parameters | Train Loss | Val Loss |
|-------|-----------|------------|----------|
| GPT-2 | 124M | 3.11 | 3.12 |
| GPT-2 Medium | 350M | 2.85 | 2.84 |
| GPT-2 Large | 774M | 2.66 | 2.67 |
| GPT-2 XL | 1558M | 2.56 | 2.54 |

<Tip>
nanoGPT can reproduce the GPT-2 124M baseline, achieving ~2.85 validation loss after finetuning on OpenWebText.
</Tip>

## Architecture Highlights

From `model.py`, the GPT implementation includes:

- **Causal Self-Attention** with Flash Attention support (PyTorch >= 2.0)
- **Layer Normalization** with optional bias
- **Transformer Blocks** with residual connections
- **Configurable Architecture** - easily adjust layers, heads, embedding dimensions
- **GPT-2 Weight Loading** - initialize from OpenAI's pretrained checkpoints

## Quick Start Options

<CardGroup cols={2}>
  <Card title="Quickstart Tutorial" icon="rocket" href="/quickstart">
    Train a character-level GPT on Shakespeare in under 5 minutes
  </Card>
  
  <Card title="Installation Guide" icon="download" href="/installation">
    Set up your environment with all required dependencies
  </Card>
  
  <Card title="Training Guide" icon="graduation-cap" href="/training">
    Learn how to train GPT models from scratch or finetune existing ones
  </Card>
  
  <Card title="API Reference" icon="book" href="/api-reference">
    Detailed documentation of model architecture and training parameters
  </Card>
</CardGroup>

## Community & Support

For questions and discussions, join the **#nanoGPT** channel on Discord.

All nanoGPT experiments are powered by GPUs on [Lambda labs](https://lambdalabs.com).

## What's Next?

Ready to get started? Head over to the [Installation Guide](/installation) to set up your environment, or jump straight to the [Quickstart](/quickstart) to train your first model!